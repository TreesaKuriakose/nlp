# -*- coding: utf-8 -*-
"""nlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cwcBrWo9AH6GSCSGSlvlfm9MGiDdcqKA
"""

import nltk

import spacy

sentence = ["I love learning Natural Language Processing with Python!"]

tokennize = [doc.lower().split()for doc in sentence]

tokennize

nltk.download('punkt')

from nltk.tokenize import word_tokenize

# prompt: tokenization using word_tokenize

sentence = "I love learning Natural Language Processing with Python!"
tokennize = word_tokenize(sentence)
tokennize

# prompt: tokenization using spacy

# If you haven't downloaded the English language model, do so by uncommenting the next line
# !python -m spacy download en_core_web_sm

import spacy

# Load the English language model
nlp = spacy.load("en_core_web_sm")

sentence = "I love learning Natural Language Processing with Python!"

# Process the sentence with SpaCy
doc = nlp(sentence)

# Extract tokens
spacy_tokens = [token.text for token in doc]
spacy_tokens

# prompt: import stemming and do

from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

stemmed_tokens = [stemmer.stem(token) for token in spacy_tokens]
stemmed_tokens

# prompt: import lemmatization and do

nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

# Lemmatize the tokens obtained from SpaCy processing
lemmatized_tokens = [lemmatizer.lemmatize(token) for token in spacy_tokens]
lemmatized_tokens

# prompt: import stop word removal and do

nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

# Remove stop words from the list of tokens (using Spacy tokens here)
filtered_tokens = [token for token in spacy_tokens if token not in stop_words]
filtered_tokens